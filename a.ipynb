{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from itertools import combinations\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairs(num_pairs=5, num_speakers=60, data_path='AudioMNIST/data'):\n",
    "    speaker_folders = [os.path.join(data_path, folder) for folder in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, folder))][:num_speakers]\n",
    "\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "\n",
    "    for this_speaker_folder in speaker_folders:\n",
    "        files1 = [os.path.join(this_speaker_folder, f) for f in os.listdir(this_speaker_folder)]\n",
    "        \n",
    "        positive_pairs += random.sample(list(combinations(files1, 2)), num_pairs * (num_speakers - 1))\n",
    "        \n",
    "        for other_speaker_folder in set(speaker_folders) - {this_speaker_folder}:\n",
    "            files2 = [os.path.join(other_speaker_folder, f) for f in os.listdir(other_speaker_folder)]\n",
    "            negative_pairs += random.sample([(f1, f2) for f1 in files1 for f2 in files2], num_pairs)\n",
    "\n",
    "    return [(pair, 1) for pair in positive_pairs] + [(pair, 0) for pair in negative_pairs]\n",
    "\n",
    "\n",
    "pairs = build_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, n_mfcc=39):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs = np.mean(mfccs.T, axis=0)  # Taking mean across time\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []\n",
    "x2 = []\n",
    "y = []\n",
    "\n",
    "for (file1, file2), label in pairs:\n",
    "    x1.append(extract_mfcc(file1))\n",
    "    x2.append(extract_mfcc(file2))\n",
    "    y.append(label)\n",
    "\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "y = np.array(y)\n",
    "\n",
    "data = list(zip(x1, x2, y))\n",
    "random.shuffle(data)\n",
    "split_idx = int(0.8 * len(data))\n",
    "train, test = data[:split_idx], data[split_idx:]\n",
    "\n",
    "x1_train, x2_train, y_train = map(np.array, zip(*train))\n",
    "x1_test, x2_test, y_test = map(np.array, zip(*test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\", input_shape=(39,)),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "nn.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(tf.abs(x1_train - x2_train), y_train, epochs=100)\n",
    "nn.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nn.evaluate(tf.abs(x1_test - x2_test), y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
